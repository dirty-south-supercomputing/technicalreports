% arara: xelatex: {shell: true}
% arara: biber
% arara: xelatex: {shell: true}
% arara: xelatex: {shell: true}
\documentclass[letterpaper,10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[justification=centering,font=small,labelfont=bf]{caption}
\usepackage{fancyhdr}
\usepackage{fontspec}
\setmonofont{Inconsolata}[Scale=MatchLowercase]
\defaultfontfeatures{Ligatures=TeX}
\usepackage{xeCJK}
\usepackage{minted}
\usepackage[backend=biber,
date=iso,
seconds=true,
style=numeric,
bibencoding=utf8,
]{biblatex}

\addbibresource{\jobname.bib}
\usemintedstyle{colorful}
\newenvironment{denseitemize}{
  \begin{itemize}
      \setlength{\itemsep}{0pt}
}{
  \end{itemize}
}

\pagestyle{fancy}
\rhead{DSSCAW Technical Report \#002}

\title{µnandfs:\\
A NAND Blobstore for Memory-Starved Platforms\thanks{
 \href{https://www.dsscaw.com/}{Dirty South Supercomputing} on behalf
 of \href{https://www.vakaros.com/}{Vakaros} of Atlanta, GA.
}\\
}
\author{Nick Black, Consulting Scientist\\
\texttt{nickblack@linux.com}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
\thispagestyle{fancy}
\date{}
\begin{abstract}
I was tasked with designing and implementing a persistent associative array
mapping names to arbitrary data---i.e. a single-directory filesystem, often
called a \textit{blobstore}---using the Nordic Semiconductor nRF52840 and two
Winbond W25N01GV gigabit SLC NAND chips. The contract also required necessary
QSPI drivers. The requirements permitted 4KB of RAM, permitted 4KB of RAM,
allowed no use of other persistent storage, and mandated a fully asynchronous
API running on ``bare metal'' (no OS, realtime or otherwise). I detail my
resulting deliverable, µnandfs, and demonstrate its generally performant
and robust fulfillment of these specs. I also describe its pathological worst
case behaviors.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
The client's initial request was simply ``a filesystem on the nRF52840 using
two W25N01GV NANDs, plus any necessary drivers, plus an entirely asynchronous
C++ API, in as little RAM as possible''. Refinement of these requirements
determined that:
\begin{denseitemize}
\item Most files would be on the order of 1KB, with a few files
       on the order of tens of megabytes. These larger files would grow
       over time, via substantial (page-sized) appends.
\item It was not thought necessary to have directories, nor links either hard
       or symbolic, but it must be possible to remove files, reclaiming both their space and
       name.
\item It must be possible to have multiple files open at once, but it is not
       necessary that a single file support multiple open handles. Writing
       to a file need not be visible to an existing reader.
\item No more than 4KB of RAM was to be consumed, and ideally not more than 2KB
       would be persistently consumed. Callers could be required to supply
       an additional 2KB for the duration of their operation.
\item Wear-leveling must be as close to uniform as possible. Ideally, no block
       would be erased two times more than any other block. Robustness in
       general is at a premium.
\end{denseitemize}

The nRF52840\parencite{nrf52840} SoC pairs an ARM Cortex-M4F with 1MB of
NOR flash and 256KB of RAM, along with a wealth of interconnection
capabilities. This storage is shared with the ``S140
SoftDevice''\parencite{s140}, a closed-source BlueTooth stack, which consumes
slightly more than 100KB of RAM and significant flash.
Two Winbond W25N01GV\parencite{winbond}
128MB NANDs, each capable of QSPI at up to 104MHz, were added to the PCB.
One QSPI and three SPI masters are available, each clocked at 32MHz (ignoring
overhead, 32MHz QSPI moves an ideal 16MB/s, SPI a respectable 4MB/s). Nordic's nRF5 SDK\parencite{nrf52sdk}
version 15.3.0 was linked into our binary, and the DUT was probed via 10-pin
J-Link\parencite{segger} connection from an nRF52-DK\parencite{nrf52dk}.

\section{Details of NAND flash}

NAND flash---typically packaged as a collection of chips, and often managed via
an on-board controller---makes up the majority of modern solid state drives,
flash drives, and memory cards. It is cheaper and denser than NOR flash,
faster than spinning disk, \textit{much} faster than EEPROMs, but typically
less reliable than all three. There are severe constraints on how it can be
used: a chip of NAND flash is divided into some number of blocks, which are
themselves divided into pages. Erasing a block changes all bits within to
1s\footnote{Nothing about the NAND memory cell itself---floating-gate MOSFETs
connected in series---requires large-scale operations. Larger blocks mean
faster operations (per bit), cheaper chips, and less power draw\ldots plus
amplified errors, and reduced flexibility.}. Data can be written a page at a
time within a block, but usually only in ascending page order---out-of-order
page programming can upset data in adjacent wordlines (a ``program
interference``)\parencite{interference}. Data can be read a page at a time from
anywhere within the block, but reading a page too many times can upset data in
an adjacent page (a ``read disturb''). SLC NAND flash stores one bit per cell,
and blocks can be erased on the order of $10^5$ times. MLC and TLC variants
encode more bits per cell, and can be reliably erased far fewer times (they
also tend to be slower, and to require more ECC bits per data bit). As a block
wears out, programming time will decrease, while erase latency
increases\parencite{needtoknow}. Finally, it is common for MLC and TLC NAND to
exhibit ``fast pages'' and ``slow pages'', but this arises from the
distribution of bits within a multilevel cell to different pages, and thus does
not affect SLC NAND\parencite{anomalies} such as our Winbond.

\subsection{Details of the Winbond W25N01GV}
The W25N01GVxxIG/IT\footnote{The ``xx'' is a package code, one of ZE (WSON), SF (SOIC),
or TB/TC (TFBGA). IG/IT differentiates between devices which reset into ``continuous mode''
or ``buffered mode''. We always use the (default) buffered mode on our IGs.} is
organized as $2^{10}$ blocks of $2^6$ 2KB pages each ($2^{17}$ bytes per block), for
a total of $2^{16}$ pages and $2^{27}$ data bytes (128MB). Blocks must be
erased before their pages can be programmed with 0s, and pages must be
programmed in ascending order. A page may be read from any block at any time.
In addition to the 2KB of data, each page has a 64 byte ``Spare Area'',
primarily used for hardware-managed ECC\footnote{Those who long to live
dangerously can disable hardware ECC. We don't.}, but offering 16 ECC-protected bytes
(in 4 discontiguous 4-byte chunks) for our use. A bad block mapper (BBM) of 20
LUTs is at our disposal, providing transparent remapping of block addresses.
Finally, there are 10 ``One-Time Program'' pages, which can be written to only
once.

Block erasure operations can fail, setting the FAIL-E bit; likewise page
programming and FAIL-P. Both of these bits are reset when the next operation
is started. Page read operations that fail their ECC check result in one or
more ECC error bits being set. These error bits remain high until the chip is
reset. Most operations set the BUSY bit, and only when this bit goes low is it
safe to consider an operation completed.

\subsection{Interactions with the nRF52840}
The Winbond's command set bears no resemblance to the ONFI
standard\parencite{onfi}, so there's no need to learn the latter. Unfortunately,
it also differs extensively from that supported by the nRF52840's QSPI
interface. Extensive use of the {\texttt{CINSTR}} ``custom instruction''
facility was needed, but eventually full-speed communication was had over
QSPI with the two NANDs. Alas, the Nordic unit can only deal with pages of
either 256 or 512 bytes. Using the maximum 512 byte setting, the last 512 bytes
written anywhere in the page would be returned for subsequent reads from the
page of any length. Eventually, it was confirmed with Nordic support that their
QSPI controller simply can't drive a 2KB page for more than 25\% of its
capacity. A bit-banging approach, even were it possible to do robustly,
would consume far too many CPU cycles (remember, all interfaces were to be
asynchronous). Faced with either a 75\% reduction in speed or a 75\% reduction
in capacity, the clients chose capacity, and we moved to the SPI interfaces.
Note that hanging both chips off the QSPI interface meant that only one would
be used at a time (aside from a mirrored configuration employing
``superblocks''\parencite{superblocks}), but placing the two on distinct SPI
interfaces allows for parallelism. Full utilization of both SPIs would represent
50\% of the original QSPI capacity.

\section{Blobstore API}
I had some ideas regarding the filesystem design, taking inspiration from
classic work on log-structured filesystems\parencite{sprite}, particularly
as implemented in NAND-focused projects such as F2FS\parencite{f2fs},
JFFS2\parencite{jffs}, and YAFFS\parencite{yaffs}. At the same time, I wanted
a simpler scheme, one implementable in 2KB of RAM, by one engineer, in four
weeks of part-time work. These ideas informed the proposed API, which happily
ended up pretty well-suited to the final implementation.

\begin{listing}[ht]
\caption{{\texttt{VK\_FS}} class public API ({\texttt{NANDDev}} defines CS pin and SPI device)}
\begin{minted}{C++}
constexpr auto BLOB_NAME_MAXLEN = 58; // Including mandatory NUL byte!
constexpr auto NAND_PAGE_DATABYTES = 2048;
constexpr auto NAND_PAGE_SPAREBYTES = 64;
constexpr auto NAND_PAGE_SIZE = NAND_PAGE_DATABYTES + NAND_PAGE_SPAREBYTES;

using blob_t = uint32_t;
using NANDPage = std::array<uint8_t, NAND_PAGE_SIZE>;
using Handler = void(*)(void *, int);
using FSCallback = void(*)(void*, blob_t, size_t);

class VK_FS {
 static int Format(Handler fxn, void* vctx, NANDDev* nand);
 template<class It> int Init(Handler fxn, void* vctx, It b, It e, bool mirror);
 int Fsck(Handler fxn, void* vctx, NANDDev* nand);
 // flags include BLOB_CREAT, BLOB_EXCL, and BLOB_KILL
 blob_t OpenBlob(const std::string& name, unsigned flags,
                 FSCallback cb, void* vctx);
 blob_t ExtendBlob(blob_t blob, const void* buf, size_t len,
                   FSCallback cb, void* vctx, NANDPage& scratch);
 blob_t ReplaceNamedBlob(const std::string& name, const void* buf, size_t len,
                         FSCallback cb, void* vctx);
 blob_t RemoveNamedBlob(const std::string& name, const void* buf, size_t len,
                        FSCallback cb, void* vctx);
 blob_t ReadBlob(blob_t blob, void* buf, size_t len, unsigned offset,
                 FSCallback cb, void* vctx, NANDPage& scratch);
 blob_t Bloblen(blob_t blob, FSCallback cb, void* vctx, NANDPage& scratch);
 blob_t CloseBlob(blob_t blob, FSCallback cb, void* vctx);
 blob_t ListBlobs(blob_t blob, FSCallback cb, void* vctx, NANDPage& scratch);
 blob_t Sync(FSCallback cb, void* vctx);
}
\end{minted}
\end{listing}

A {\texttt{NANDDev}} is a small structure, often {\texttt{constexpr}}, tying
together a SPI master identifier and a Chip Select line (numerous slaves can be
connected to a single SPI master, and selected via this line). The same {\texttt{NANDDev}}
must not be provided to two {\texttt{VK\_FS}} objects. To {\texttt{Format}} a
NANDDev is simply to erase all its blocks, restoring all bits to their default
1. {\texttt{Init}} is handed one or more {\texttt{NANDDev}}s, and the boolean
{\texttt{mirror}} (if false, the NANDs are put in linear combination). All
specified devices must be freshly formatted, or they must make up an existing
filesystem (by virtue of having previously been fed together to
{\texttt{Init}})\footnote{In most mirrors, we'd want to be able to add a
  replacement device, but that won't be happening with our PCB in the field.}.

\section{Filesystem design}
%I knew I wanted to
%march purely forward writing within a chip---i.e., in addition to the mandatory
%in-order programming of pages, I hoped to erase blocks in order, with no special
%meaning for any given block. Any such scheme is guaranteed perfectly uniform
%wear leveling, and should require minimal state. I knew I needed to
%provide at least a few dozen bytes' worth of name per blob. I considered
%compression, and rejected it. I knew that any frequently-changing state
%couldn't be persisted without either redundancy or special-purpose blocks.

\section{Future work}
It is desirable to encode large files more efficiently. Currently, a file of
the maximum $2^{24}-1$ bytes requires 8192 inodes, and thus (in the best case)
530 pages of metadata, and attendant reads. When all of a zone is a single
blob, some different scheme ought be employed to encode this fact.
Unfortunately, it's unlikely that such a write could ever be performed as a
single ExtendBlob() operation, due to the RAM requirements of such a buffer.

It is not currently possible to use the two chips in parallel as a single,
unified blobstore. If placed on distinct SPI masters, they can be used in
parallel as two blobstores, or as a mirrored set. In any configuration---even
a single SPI master---they can be combined as a linear device. A unified
2Gbit namespace accessible at 2x34MHz, however, is not yet possible. Mirrored
sets could be implemented via superblocks for
maximum performance, requiring only a single SPI master. This would be especially
advantageous if the QSPI interface had been capable of driving the Winbonds.

An ECC failure reading a metadata page currently results in the blobstore being
brought offline. Given that ECC failures can often be recognized immediately
after programming, metapages should probably be read back following write,
remapped using the BBM LUTs, and written anew. In any case, a more graceful
recovery seems desirable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\printbibliography
\end{document}
